# -*- coding: utf-8 -*-
"""DeserializedNeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WK9OW-o-iFBZ3Ufu5VgmJ2Ua7BkjGldb
"""

#All imports
from helpers import preprocessing

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import pickle

#preprocessing
from sklearn.pipeline import Pipeline

#model
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
import tensorflow_addons as tfa

#constants
EMBEDDING_DIM = 100
np.random.seed(42)
tf.random.set_seed(42)

#helper function to perform initial cleanup
def prepare_data(df):
  processed_df = preprocessing.preprocess_data(df, 'comment')  
  processed_df = preprocessing.convert_emoticon(processed_df, 'spelling_corrected', False)
  processed_df['cleaned_comment'] = processed_df['emoticon_converted']
  return processed_df


#helper functions for pretrained_embeddings
#load glove embeddings
pretrained_embeddings = dict()
with open('./embeddings/glove.twitter.27B.100d.txt','r',encoding='utf-8') as file_handle:
  for line in file_handle:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:],'float32')
    pretrained_embeddings[word]=vector

#load emoji embeddings
emoji_embeddings = dict()
with open('./embeddings/emoji_embeddings_100d.txt','r',encoding='utf-8') as file_handle:
  for line in file_handle:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:],'float32')
    emoji_embeddings[word]=vector

#helper function to convert input comment to its embedded vector representation
def vocab_embeddings(word_vocab):
  num_words = len(word_vocab) + 1
  word_embeddings = np.zeros((num_words, EMBEDDING_DIM))

  for word, i in word_vocab.items():
    if word in pretrained_embeddings:
      word_embeddings[i] = pretrained_embeddings[word]
    elif word in emoji_embeddings:
      word_embeddings[i] = emoji_embeddings[word]

  return word_embeddings

#CustomTokenizer to represent deserialized pipeline element
class CustomTokenizer:
  def __init__(self):
    #initialize tf tokenizer
    self.tokenizer = Tokenizer()
  
  def _vectorize_input(self, comments):   
    comments_seq = self.tokenizer.texts_to_sequences(comments)
    comments_seq = sequence.pad_sequences(comments_seq, maxlen=self.max_length, padding='post', truncating='post')
    return comments_seq

  def fit_transform(self, comments, y=None):
    """invoked on training data to fit tokenizer and convert comments to sequences"""        
    max_length = max(comments.map(len))
    self.max_length = min(max_length, 30)
    self.tokenizer.fit_on_texts(comments)
    return self._vectorize_input(comments)    

  def transform(self, comments):
    """invoked on testing/validation data to convert comments to sequences"""
    return self._vectorize_input(comments)

  def get_embeddings(self):
    """get vocab embeddings to be fed to embedding layer"""
    word_embeddings = vocab_embeddings(self.tokenizer.word_index)
    return word_embeddings

  def get_input_dim(self):
    """input sequence dimension to be fed to embedding layer"""
    return self.max_length
  
  
#metrics for evaluation
def f1_score(true, pred):
  metric = tfa.metrics.F1Score(num_classes=4, average='weighted', threshold=0.5)
  metric.update_state(true, pred)
  result = metric.result()
  return result.numpy()

class SentimentPredictor:
    """loads the pickle file, deserialize the model and use for prediction"""
    def __init__(self):
        print("Load pipeline...")
        tokenizer = pickle.load(open('./model/tokenizer.pkl','rb'))
        build_model = lambda: tf.keras.models.load_model('./model/BidirectionalLSTM.h5', custom_objects={'f1_score':f1_score})
        classifier = KerasClassifier(build_fn=build_model)
        classifier.classes_ = pickle.load(open('./model/classes.pkl','rb'))
        classifier.model = build_model()
        self.pipeline = Pipeline([('tokenize', tokenizer), ('model', classifier)])
        self.target_labels = {0:'Neutral', 1:'Positive', 2:'Negative', 3:'Mixed'}        
        
    def predict(self, comment):
        df = pd.DataFrame({'comment':[comment]})
        df = prepare_data(df)
        cleaned_comment = df['cleaned_comment']
        predicted_probs = list(self.pipeline.predict_proba(cleaned_comment)[0])
        res = dict()
        for idx, prob in enumerate(predicted_probs):
            res[self.target_labels[idx]] = prob
        
        return res
        

