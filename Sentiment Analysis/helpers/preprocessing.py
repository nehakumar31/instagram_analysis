# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gdGeX5z6lUnTwdC2_pk3QXn2v1pVLJa2

**Contains all preprocessing functions needed for data preparation**
"""

#All imports
import re
import string

import nltk
from nltk.tokenize import TweetTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import stopwords

from spellchecker import SpellChecker

#import spacy
#from spacy_langdetect import LanguageDetector
from emoji import UNICODE_EMOJI
#import pycld2 as cld2

import contractions
from textblob import Word

#All downloads
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')
#spacy_nlp = spacy.load('en')

ENG_EMOJI = UNICODE_EMOJI['en']
ALL_STOPWORDS = stopwords.words('english')
SUB_STOPWORDS = []

abbreviations = {
    "$" : ["dollar"],
    "€" : ["euro"],
    "4ao" : ["for","adults","only"],
    "a.m" : ["before","midday"],
    "a3" : ["anytime","anywhere","anyplace"],
    "aamof" : ["as","a","matter","of","fact"],
    "acct" : ["account"],
    "adih" : ["another","day","in","hell"],
    "afaic" : ["as","far","as","i","am","concerned"],
    "afaict" : ["as","far","as","i","can","tell"],
    "afaik" : ["as","far","as","i","know"],
    "afair" : ["as","far","as","i","remember"],
    "afk" : ["away","from","keyboard"],
    "app" : ["application"],
    "approx" : ["approximately"],
    "apps" : ["applications"],
    "asap" : ["as","soon","as","possible"],
    "asl" : ["age,","sex,","location"],
    "atk" : ["at","the","keyboard"],
    "atm" : ["at","the","moment"],
    "ave." : ["avenue"],
    "aymm" : ["are","you","my","mother"],
    "ayor" : ["at","your","own","risk"],
    "abs" : ["absolutely"],
    "b&b" : ["bed","and","breakfast"],
    "b+b" : ["bed","and","breakfast"],
    "b.c" : ["before","christ"],
    "b2b" : ["business","to","business"],
    "b2c" : ["business","to","customer"],
    "b4" : ["before"],
    "b4n" : ["bye","for","now"],
    "b@u" : ["back","at","you"],
    "bae" : ["before","anyone","else"],
    "bak" : ["back","at","keyboard"],
    "bbbg" : ["bye","bye","be","good"],
    "bbc" : ["british","broadcasting","corporation"],
    "bbias" : ["be","back","in","a","second"],
    "bbl" : ["be","back","later"],
    "bbs" : ["be","back","soon"],
    "be4" : ["before"],    
    "bc" : ["because"],
    "bfn" : ["bye","for","now"],    
    "blvd" : ["boulevard"],
    "bout" : ["about"],
    "brb" : ["be","right","back"],
    "bros" : ["brothers"],
    "brt" : ["be","right","there"],
    "bsaaw" : ["big","smile","and","a","wink"],
    "btw" : ["by","the","way"],
    "bwl" : ["bursting","with","laughter"],
    "blm" : ["black","lives","matter"],
    "c/o" : ["care","of"],
    "cet" : ["central","european","time"],
    "cf" : ["compare"],
    "cia" : ["central","intelligence","agency"],
    "csl" : ["can","not","stop","laughing"],
    "cu" : ["see","you"],
    "cul8r" : ["see","you","later"],
    "cv" : ["curriculum","vitae"],
    "cwot" : ["complete","waste","of","time"],
    "cya" : ["see","you"],
    "cyt" : ["see","you","tomorrow"],
    "dae" : ["does","anyone","else"],
    "dbmib" : ["do","not","bother","me","i","am","busy"],
    "diy" : ["do","it","yourself"],
    "dm" : ["direct","message"],
    "dwh" : ["during","work","hours"],
    "e123" : ["easy","as","one","two","three"],
    "eet" : ["eastern","european","time"],
    "eg" : ["example"],
    "embm" : ["early","morning","business","meeting"],
    "encl" : ["enclosed"],
    "encl." : ["enclosed"],
    "etc" : ["and","so","on"],
    "faq" : ["frequently","asked","questions"],
    "fawc" : ["for","anyone","who","cares"],
    "fb" : ["facebook"],
    "fc" : ["fingers","crossed"],
    "fig" : ["figure"],
    "fimh" : ["forever","in","my","heart"],
    "ft." : ["feet"],
    "ft" : ["featuring"],
    "ftl" : ["for","the","loss"],
    "ftw" : ["for","the","win"],
    "fwiw" : ["for","what","it","is","worth"],
    "fyi" : ["for","your","information"],
    "g9" : ["genius"],
    "gahoy" : ["get","a","hold","of","yourself"],
    "gal" : ["get","a","life"],
    "gcse" : ["general","certificate","of","secondary","education"],
    "gfn" : ["gone","for","now"],
    "gg" : ["good","game"],
    "gl" : ["good","luck"],
    "glhf" : ["good","luck","have","fun"],
    "gmt" : ["greenwich","mean","time"],
    "gmta" : ["great","minds","think","alike"],
    "gn" : ["good","night"],
    "g.o.a.t" : ["greatest","of","all","time"],
    "goat" : ["greatest","of","all","time"],
    "goi" : ["get","over","it"],
    "gps" : ["global","positioning","system"],
    "gr8" : ["great"],
    "gratz" : ["congratulations"],
    "gyal" : ["girl"],
    "h&c" : ["hot","and","cold"],
    "hp" : ["horsepower"],
    "hr" : ["hour"],
    "hrh" : ["his","royal","highness"],
    "ht" : ["height"],
    "ibrb" : ["i","will","be","right","back"],
    "ic" : ["i","see"],
    "icq" : ["i","seek","you"],
    "icymi" : ["in","case","you","missed","it"],
    "idc" : ["i","do","not","care"],
    "idgadf" : ["i","do","not","give","a","damn","fuck"],
    "idgaf" : ["i","do","not","give","a","fuck"],
    "idk" : ["i","do","not","know"],
    "ie" : ["that","is"],
    "i.e" : ["that","is"],
    "ifyp" : ["i","feel","your","pain"],
    "IG" : ["instagram"],
    "iirc" : ["if","i","remember","correctly"],
    "ilu" : ["i","love","you"],
    "ily" : ["i","love","you"],
    "imho" : ["in","my","humble","opinion"],
    "imo" : ["in","my","opinion"],
    "imu" : ["i","miss","you"],
    "iow" : ["in","other","words"],
    "irl" : ["in","real","life"],
    "ilysm" : ["i","love","you", "so", "much"],
    "j4f" : ["just","for","fun"],
    "jic" : ["just","in","case"],
    "jk" : ["just","kidding"],
    "jsyk" : ["just","so","you","know"],
    "kiss" : ["keep","it","simple","stupid"],
    "l8r" : ["later"],
    "lb" : ["pound"],
    "lbs" : ["pounds"],
    "ldr" : ["long","distance","relationship"],
    "lmao" : ["laugh","my","ass","off"],
    "lmfao" : ["laugh","my","fucking","ass","off"],
    "lol" : ["laughing","out","loud"],
    "ltd" : ["limited"],
    "ltns" : ["long","time","no","see"],
    "m8" : ["mate"],
    "mf" : ["motherfucker"],
    "mfs" : ["motherfuckers"],
    "mfw" : ["my","face","when"],
    "mofo" : ["motherfucker"],
    "mph" : ["miles","per","hour"],
    "mr" : ["mister"],
    "mrw" : ["my","reaction","when"],
    "ms" : ["miss"],
    "mte" : ["my","thoughts","exactly"],
    "nagi" : ["not","a","good","idea"],
    "nbc" : ["national","broadcasting","company"],
    "nbd" : ["not","big","deal"],
    "nfs" : ["not","for","sale"],
    "ngl" : ["not","going","to","lie"],
    "nhs" : ["national","health","service"],
    "nrn" : ["no","reply","necessary"],
    "nsfl" : ["not","safe","for","life"],
    "nsfw" : ["not","safe","for","work"],
    "nth" : ["nice","to","have"],
    "nvr" : ["never"],
    "nyc" : ["new","york","city"],
    "oc" : ["original","content"],
    "og" : ["original"],
    "ohp" : ["overhead","projector"],
    "oic" : ["oh","i","see"],
    "omdb" : ["over","my","dead","body"],
    "omg" : ["oh","my","god"],
    "omw" : ["on","my","way"],
    "otw" : ["on","the","way"],
    "oos" : ["out","of","stock"],
    "p.a" : ["per","annum"],
    "p.m" : ["after","midday"],
    "pm" : ["prime","minister"],
    "poc" : ["people","of","color"],
    "pov" : ["point","of","view"],
    "pic" : ["picture"],
    "pp" : ["price","please"],
    "ppl" : ["people"],
    "prw" : ["parents","are","watching"],
    "ps" : ["postscript"],
    "pt" : ["point"],
    "ptb" : ["please","text","back"],
    "pto" : ["please","turn","over"],  
    "pita" : ["pain","in","the","ass"],
    "prt" : ["party"],
    "plz" : ["please"],
    "pls" : ["please"],
    "pst" : ["pacific", "time", "zone"],
    "ratchet" : ["rude"],
    "rbtl" : ["read","between","the","lines"],
    "rlrt" : ["real","life","retweet"],
    "rofl" : ["rolling","on","the","floor","laughing"],
    "roflol" : ["rolling","on","the","floor","laughing","out","loud"],
    "rotflmao" : ["rolling","on","the","floor","laughing","my","ass","off"],
    "rt" : ["retweet"],
    "ruok" : ["are","you","ok"],
    "sfw" : ["safe","for","work"],
    "sk8" : ["skate"],
    "smh" : ["shake","my","head"],
    "sq" : ["square"],
    "srsly" : ["seriously"],
    "ssdd" : ["same","stuff","different","day"],
    "tbh" : ["to","be","honest"],
    "tbs" : ["tablespooful"],
    "tbsp" : ["tablespooful"],
    "tfw" : ["that","feeling","when"],
    "thks" : ["thank","you"],
    "tho" : ["though"],
    "thx" : ["thank","you"],
    "tia" : ["thanks","in","advance"],
    "til" : ["today","i","learned"],
    "tl;dr" : ["too","long","i","did","not","read"],
    "tldr" : ["too","long","i","did","not","read"],
    "tmb" : ["tweet","me","back"],
    "tntl" : ["trying","not","to","laugh"],
    "ttyl" : ["talk","to","you","later"],
    "ttfn" : ["bye", "for", "now"],
    "ty" : ["thank","you"],
    "u" : ["you"],
    "u2" : ["you","too"],
    "u4e" : ["yours","for","ever"],
    "utc" : ["coordinated","universal","time"],
    "uk" : ["united","kingdom"],
    "w/" : ["with"],
    "w/o" : ["without"],
    "w8" : ["wait"],
    "wassup" : ["what","is","up"],
    "wb" : ["welcome","back"],
    "wtf" : ["what","the","fuck"],
    "wtg" : ["way","to","go"],
    "wtpa" : ["where","the","party","at"],
    "wuf" : ["where","are","you","from"],
    "wuzup" : ["what","is","up"],
    "wywh" : ["wish","you","were","here"],
    "x" : ["kiss"],
    "yd" : ["yard"],
    "ygtr" : ["you","got","that","right"],
    "ynk" : ["you","never","know"],
    "zzz" : ["sleeping","bored","and","tired"]
}

urban_slangs_patt = {
    "^w+o+w+$" : "surprised",
    "^y+a+s+$" : "yes",
    "^omg+$" : "surprised",
    "^ho+ly$" : "surprised",
    "^o+h+$" : "surprised",
    "^a+y+$" : "happiness",
    "^bo+ring+$" : "boring",
    "^so+$$" : "so",
    "^mo+l+y+$" : "surprised",
    "^fav+e*s*$" : "favourite",
    "^babes*$" : "girl",
    "^ye+s+$" : "yes",
    "^a+w+$" : "beautiful",
    "^cut+e+$" : "cute",
    "^pret+y+$" : "pretty",
    "^lo+v+e+$" : "love",
    "^m+y+$" : "my",
    "^a+h+$" : "surprised",
    "^ho+w+$" : "how",
    "^gorg+$" : "gorgeous",
    "^lmao+$" : "lmao",
    "^mu+c+h+$" : "much",
    "^go{2,}d+$" : "good",
    "^gosh$" : "god",
    "^lu+v+$" : "luv",
    "^ug+h+$" : "disappointed",
    "^t+y+$" : "ty",
    "^cu+t+e+$" : "cute",
    "^omw+$" : "omw",
    "^glam+$" : "glamorous",
    "^nope+s*$" : "no",
    "^cuz$" : "because",
    "^bcoz$" : "because",
    "^ne{2,}d+z*$" : "need",
    "^de{2,}t+s+$" : "details",
    "^wo+w+z+a+$" : "wow",
    "^vibes*z*$" : "feelings",
    "freaking" : "exceptionally",
    "^al{2,}$" : "all",
    "^flawles+$" : "perfect",
    "^o+$" : "wow",    
    "^bes+t+$" : "best",
    "^haha+$" : "laughing",
    "^booo+$" : "displeasure",
    "^guy+s*$" : "people" ,
    "^hi{1,}$" : "hi",
    "^gon+a+$" : "going",
    "^kids*$" : "children",
    "^ya+y+$" : "excited",
    "^gu+r+l+$" : "girl",
    "^o+u+$" : "wow",
    "^ko{2,}l$" : "cool",
    "gotta" : "got",
    "^wao+$" : "wow",
    "^bot+h+$" : "both",
    "^thi+s+$" : "this",
    "^sis$" : "sister",
    "^(omg){1,}$" : "omg",
    "^yep+$" : "yes",
    "^gi+r+l+$" : "girl",
    "^ye+s+$" : "yes",
    "^ple+a+s+e+$" : "please",
    "^bye+$" : "bye",
    "^plz+$" : "please",
    "^wo+$" : "wow",
    "^dm+$" : "dm"
}

EMOTICONS = {
    u":‑\)":"Happy face or smiley",
    u":\)":"Happy face or smiley",
    u":-\]":"Happy face or smiley",
    u":\]":"Happy face or smiley",
    u":-3":"Happy face smiley",
    u":3":"Happy face smiley",
    u":->":"Happy face smiley",
    u":>":"Happy face smiley",
    u"8-\)":"Happy face smiley",
    u":o\)":"Happy face smiley",
    u":-\}":"Happy face smiley",
    u":\}":"Happy face smiley",
    u":-\)":"Happy face smiley",
    u":c\)":"Happy face smiley",
    u":\^\)":"Happy face smiley",
    u"=\]":"Happy face smiley",
    u"=\)":"Happy face smiley",
    u":‑D":"Laughing, big grin or laugh with glasses",
    u":D":"Laughing, big grin or laugh with glasses",
    u"8‑D":"Laughing, big grin or laugh with glasses",
    u"8D":"Laughing, big grin or laugh with glasses",
    u"X‑D":"Laughing, big grin or laugh with glasses",
    u"XD":"Laughing, big grin or laugh with glasses",
    u"=D":"Laughing, big grin or laugh with glasses",
    u"=3":"Laughing, big grin or laugh with glasses",
    u"B\^D":"Laughing, big grin or laugh with glasses",
    u":-\)\)":"Very happy",
    u":‑\(":"Frown, sad, andry or pouting",
    u":-\(":"Frown, sad, andry or pouting",
    u":\(":"Frown, sad, andry or pouting",
    u":‑c":"Frown, sad, andry or pouting",
    u":c":"Frown, sad, andry or pouting",
    u":‑<":"Frown, sad, andry or pouting",
    u":<":"Frown, sad, andry or pouting",
    u":‑\[":"Frown, sad, andry or pouting",
    u":\[":"Frown, sad, andry or pouting",
    u":-\|\|":"Frown, sad, andry or pouting",
    u">:\[":"Frown, sad, andry or pouting",
    u":\{":"Frown, sad, andry or pouting",
    u":@":"Frown, sad, andry or pouting",
    u">:\(":"Frown, sad, andry or pouting",
    u":'‑\(":"Crying",
    u":'\(":"Crying",
    u":'‑\)":"Tears of happiness",
    u":'\)":"Tears of happiness",
    u"D‑':":"Horror",
    u"D:<":"Disgust",
    u"D:":"Sadness",
    u"D8":"Great dismay",
    u"D;":"Great dismay",
    u"D=":"Great dismay",
    u"DX":"Great dismay",
    u":‑O":"Surprise",
    u":O":"Surprise",
    u":‑o":"Surprise",
    u":o":"Surprise",
    u":-0":"Shock",
    u"8‑0":"Yawn",
    u">:O":"Yawn",
    u":-\*":"Kiss",
    u":\*":"Kiss",
    u":X":"Kiss",
    u";‑\)":"Wink or smirk",
    u";\)":"Wink or smirk",
    u"\*-\)":"Wink or smirk",
    u"\*\)":"Wink or smirk",
    u";‑\]":"Wink or smirk",
    u";\]":"Wink or smirk",
    u";\^\)":"Wink or smirk",
    u":‑,":"Wink or smirk",
    u";D":"Wink or smirk",
    u":‑P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"X‑P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"XP":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":‑Þ":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":Þ":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":b":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"d:":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"=p":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u">:P":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u":‑/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":-[.]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u">:[(\\\)]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u">:/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":[(\\\)]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u"=/":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u"=[(\\\)]":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":L":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u"=L":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":S":"Skeptical, annoyed, undecided, uneasy or hesitant",
    u":‑\|":"Straight face",
    u":\|":"Straight face",
    u":$":"Embarrassed or blushing",
    u":‑x":"Sealed lips or wearing braces or tongue-tied",
    u":x":"Sealed lips or wearing braces or tongue-tied",
    u":‑#":"Sealed lips or wearing braces or tongue-tied",
    u":#":"Sealed lips or wearing braces or tongue-tied",
    u":‑&":"Sealed lips or wearing braces or tongue-tied",
    u":&":"Sealed lips or wearing braces or tongue-tied",
    u"O:‑\)":"Angel, saint or innocent",
    u"O:\)":"Angel, saint or innocent",
    u"0:‑3":"Angel, saint or innocent",
    u"0:3":"Angel, saint or innocent",
    u"0:‑\)":"Angel, saint or innocent",
    u"0:\)":"Angel, saint or innocent",
    u":‑b":"Tongue sticking out, cheeky, playful or blowing a raspberry",
    u"0;\^\)":"Angel, saint or innocent",
    u">:‑\)":"Evil or devilish",
    u">:\)":"Evil or devilish",
    u"\}:‑\)":"Evil or devilish",
    u"\}:\)":"Evil or devilish",
    u"3:‑\)":"Evil or devilish",
    u"3:\)":"Evil or devilish",
    u">;\)":"Evil or devilish",
    u"\|;‑\)":"Cool",
    u"\|‑O":"Bored",
    u":‑J":"Tongue-in-cheek",
    u"#‑\)":"Party all night",
    u"%‑\)":"Drunk or confused",
    u"%\)":"Drunk or confused",
    u":-###..":"Being sick",
    u":###..":"Being sick",
    u"<:‑\|":"Dump",
    u"\(>_<\)":"Troubled",
    u"\(>_<\)>":"Troubled",
    u"\(';'\)":"Baby",
    u"\(\^\^>``":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(\^_\^;\)":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(-_-;\)":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(~_~;\) \(・\.・;\)":"Nervous or Embarrassed or Troubled or Shy or Sweat drop",
    u"\(-_-\)zzz":"Sleeping",
    u"\(\^_-\)":"Wink",
    u"\(\(\+_\+\)\)":"Confused",
    u"\(\+o\+\)":"Confused",
    u"\(o\|o\)":"Ultraman",
    u"\^_\^":"Joyful",
    u"\(\^_\^\)/":"Joyful",
    u"\(\^O\^\)／":"Joyful",
    u"\(\^o\^\)／":"Joyful",
    u"\(__\)":"Kowtow as a sign of respect, or dogeza for apology",
    u"_\(\._\.\)_":"Kowtow as a sign of respect, or dogeza for apology",
    u"<\(_ _\)>":"Kowtow as a sign of respect, or dogeza for apology",
    u"<m\(__\)m>":"Kowtow as a sign of respect, or dogeza for apology",
    u"m\(__\)m":"Kowtow as a sign of respect, or dogeza for apology",
    u"m\(_ _\)m":"Kowtow as a sign of respect, or dogeza for apology",
    u"\('_'\)":"Sad or Crying",
    u"\(/_;\)":"Sad or Crying",
    u"\(T_T\) \(;_;\)":"Sad or Crying",
    u"\(;_;":"Sad of Crying",
    u"\(;_:\)":"Sad or Crying",
    u"\(;O;\)":"Sad or Crying",
    u"\(:_;\)":"Sad or Crying",
    u"\(ToT\)":"Sad or Crying",
    u";_;":"Sad or Crying",
    u";-;":"Sad or Crying",
    u";n;":"Sad or Crying",
    u";;":"Sad or Crying",
    u"Q\.Q":"Sad or Crying",
    u"T\.T":"Sad or Crying",
    u"QQ":"Sad or Crying",
    u"Q_Q":"Sad or Crying",
    u"\(-\.-\)":"Shame",
    u"\(-_-\)":"Shame",
    u"\(一一\)":"Shame",
    u"\(；一_一\)":"Shame",
    u"\(=_=\)":"Tired",
    u"\(=\^\·\^=\)":"cat",
    u"\(=\^\·\·\^=\)":"cat",
    u"=_\^=	":"cat",
    u"\(\.\.\)":"Looking down",
    u"\(\._\.\)":"Looking down",
    u"\^m\^":"Giggling with hand covering mouth",
    u"\(\・\・?":"Confusion",
    u"\(?_?\)":"Confusion",
    u">\^_\^<":"Normal Laugh",
    u"<\^!\^>":"Normal Laugh",
    u"\^/\^":"Normal Laugh",
    u"\（\*\^_\^\*）" :"Normal Laugh",
    u"\(\^<\^\) \(\^\.\^\)":"Normal Laugh",
    u"\(^\^\)":"Normal Laugh",
    u"\(\^\.\^\)":"Normal Laugh",
    u"\(\^_\^\.\)":"Normal Laugh",
    u"\(\^_\^\)":"Normal Laugh",
    u"\(\^\^\)":"Normal Laugh",
    u"\(\^J\^\)":"Normal Laugh",
    u"\(\*\^\.\^\*\)":"Normal Laugh",
    u"\(\^—\^\）":"Normal Laugh",
    u"\(#\^\.\^#\)":"Normal Laugh",
    u"\（\^—\^\）":"Waving",
    u"\(;_;\)/~~~":"Waving",
    u"\(\^\.\^\)/~~~":"Waving",
    u"\(-_-\)/~~~ \($\·\·\)/~~~":"Waving",
    u"\(T_T\)/~~~":"Waving",
    u"\(ToT\)/~~~":"Waving",
    u"\(\*\^0\^\*\)":"Excited",
    u"\(\*_\*\)":"Amazed",
    u"\(\*_\*;":"Amazed",
    u"\(\+_\+\) \(@_@\)":"Amazed",
    u"\(\*\^\^\)v":"Laughing,Cheerful",
    u"\(\^_\^\)v":"Laughing,Cheerful",
    u"\(\(d[-_-]b\)\)":"Headphones,Listening to music",
    u'\(-"-\)':"Worried",
    u"\(ーー;\)":"Worried",
    u"\(\^0_0\^\)":"Eyeglasses",
    u"\(\＾ｖ\＾\)":"Happy",
    u"\(\＾ｕ\＾\)":"Happy",
    u"\(\^\)o\(\^\)":"Happy",
    u"\(\^O\^\)":"Happy",
    u"\(\^o\^\)":"Happy",
    u"\)\^o\^\(":"Happy",
    u":O o_O":"Surprised",
    u"o_0":"Surprised",
    u"o\.O":"Surpised",
    u"\(o\.o\)":"Surprised",
    u"oO":"Surprised",
    u"\(\*￣m￣\)":"Dissatisfied",
    u"\(‘A`\)":"Snubbed or Deflated",
    u"<3":"Heart"
}

TRIMMED_EMOTICONS = {
    u":‑\)":"Happy face",
    u":\)":"Happy face",
    u":-\]":"Happy face",
    u":\]":"Happy face",
    u":-3":"Happy face",
    u":3":"Happy face",
    u":->":"Happy face",
    u":>":"Happy face",
    u"8-\)":"Happy face",
    u":o\)":"Happy face",
    u":-\}":"Happy face",
    u":\}":"Happy face",
    u":-\)":"Happy face",
    u":c\)":"Happy face",
    u":\^\)":"Happy face",
    u"=\]":"Happy face",
    u"=\)":"Happy face",
    u":‑D":"Laughing",
    u":D":"Laughing",
    u"8‑D":"Laughing",
    u"8D":"Laughing",
    u"X‑D":"Laughing",
    u"XD":"Laughing",
    u"=D":"Laughing",
    u"=3":"Laughing",
    u"B\^D":"Laughing",
    u":-\)\)":"Very happy",
    u":‑\(":"Frown",
    u":-\(":"Frown",
    u":\(":"Frown",
    u":‑c":"Frown",
    u":c":"Frown",
    u":‑<":"Frown",
    u":<":"Frown",
    u":‑\[":"Frown",
    u":\[":"Frown",
    u":-\|\|":"Frown",
    u">:\[":"Frown",
    u":\{":"Frown",
    u":@":"Frown",
    u">:\(":"Frown",
    u":'‑\(":"Crying",
    u":'\(":"Crying",
    u":'‑\)":"Tears of happiness",
    u":'\)":"Tears of happiness",
    u"D‑':":"Horror",
    u"D:<":"Disgust",
    u"D:":"Sadness",
    u"D8":"Great dismay",
    u"D;":"Great dismay",
    u"D=":"Great dismay",
    u"DX":"Great dismay",
    u":‑O":"Surprise",
    u":O":"Surprise",
    u":‑o":"Surprise",
    u":o":"Surprise",
    u":-0":"Shock",
    u"8‑0":"Yawn",
    u">:O":"Yawn",
    u":-\*":"Kiss",
    u":\*":"Kiss",
    u":X":"Kiss",
    u";‑\)":"Wink",
    u";\)":"Wink",
    u"\*-\)":"Wink",
    u"\*\)":"Wink",
    u";‑\]":"Wink",
    u";\]":"Wink",
    u";\^\)":"Wink",
    u":‑,":"Wink",
    u";D":"Wink",
    u":‑P":"playful",
    u":P":"playful",
    u"X‑P":"playful",
    u"XP":"playful",
    u":‑Þ":"playful",
    u":Þ":"playful",
    u":b":"playful",
    u"d:":"playful",
    u"=p":"playful",
    u">:P":"playful",
    u":‑/":"Skeptical",
    u":/":"Skeptical",
    u":-[.]":"Skeptical",
    u">:[(\\\)]":"Skeptical",
    u">:/":"Skeptical",
    u":[(\\\)]":"Skeptical",
    u"=/":"Skeptical",
    u"=[(\\\)]":"Skeptical",
    u":L":"Skeptical",
    u"=L":"Skeptical",
    u":S":"Skeptical",
    u":‑\|":"Straight face",
    u":\|":"Straight face",
    u":$":"Embarrassed",
    u":‑x":"Sealed lips",
    u":x":"Sealed lips",
    u":‑#":"Sealed lips",
    u":#":"Sealed lips",
    u":‑&":"Sealed lips",
    u":&":"Sealed lips",
    u"O:‑\)":"innocent",
    u"O:\)":"innocent",
    u"0:‑3":"innocent",
    u"0:3":"innocent",
    u"0:‑\)":"innocent",
    u"0:\)":"innocent",
    u":‑b":"playful",
    u"0;\^\)":"innocent",
    u">:‑\)":"Evil",
    u">:\)":"Evil",
    u"\}:‑\)":"Evil",
    u"\}:\)":"Evil",
    u"3:‑\)":"Evil",
    u"3:\)":"Evil",
    u">;\)":"Evil",
    u"\|;‑\)":"Cool",
    u"\|‑O":"Bored",
    u":‑J":"Tongue-in-cheek",
    u"#‑\)":"Party all night",
    u"%‑\)":"confused",
    u"%\)":"confused",
    u":-###..":"Being sick",
    u":###..":"Being sick",
    u"<:‑\|":"Dump",
    u"\(>_<\)":"Troubled",
    u"\(>_<\)>":"Troubled",
    u"\(';'\)":"Baby",
    u"\(\^\^>``":"Nervous",
    u"\(\^_\^;\)":"Nervous",
    u"\(-_-;\)":"Nervous",
    u"\(~_~;\) \(・\.・;\)":"Nervous",
    u"\(-_-\)zzz":"Sleeping",
    u"\(\^_-\)":"Wink",
    u"\(\(\+_\+\)\)":"Confused",
    u"\(\+o\+\)":"Confused",
    u"\(o\|o\)":"Ultraman",
    u"\^_\^":"Joyful",
    u"\(\^_\^\)/":"Joyful",
    u"\(\^O\^\)／":"Joyful",
    u"\(\^o\^\)／":"Joyful",
    u"\(__\)":"Kowtow as a sign of respect",
    u"_\(\._\.\)_":"Kowtow as a sign of respect",
    u"<\(_ _\)>":"Kowtow as a sign of respect",
    u"<m\(__\)m>":"Kowtow as a sign of respect",
    u"m\(__\)m":"Kowtow as a sign of respect",
    u"m\(_ _\)m":"Kowtow as a sign of respect",
    u"\('_'\)":"Sad",
    u"\(/_;\)":"Sad",
    u"\(T_T\) \(;_;\)":"Sad",
    u"\(;_;":"Sad",
    u"\(;_:\)":"Sad",
    u"\(;O;\)":"Sad",
    u"\(:_;\)":"Sad",
    u"\(ToT\)":"Sad",
    u";_;":"Sad",
    u";-;":"Sad",
    u";n;":"Sad",
    u";;":"Sad",
    u"Q\.Q":"Sad",
    u"T\.T":"Sad",
    u"QQ":"Sad",
    u"Q_Q":"Sad",
    u"\(-\.-\)":"Shame",
    u"\(-_-\)":"Shame",
    u"\(一一\)":"Shame",
    u"\(；一_一\)":"Shame",
    u"\(=_=\)":"Tired",
    u"\(=\^\·\^=\)":"cat",
    u"\(=\^\·\·\^=\)":"cat",
    u"=_\^=	":"cat",
    u"\(\.\.\)":"Looking down",
    u"\(\._\.\)":"Looking down",
    u"\^m\^":"Giggling",
    u"\(\・\・?":"Confusion",
    u"\(?_?\)":"Confusion",
    u">\^_\^<":"Normal Laugh",
    u"<\^!\^>":"Normal Laugh",
    u"\^/\^":"Normal Laugh",
    u"\（\*\^_\^\*）" :"Normal Laugh",
    u"\(\^<\^\) \(\^\.\^\)":"Normal Laugh",
    u"\(^\^\)":"Normal Laugh",
    u"\(\^\.\^\)":"Normal Laugh",
    u"\(\^_\^\.\)":"Normal Laugh",
    u"\(\^_\^\)":"Normal Laugh",
    u"\(\^\^\)":"Normal Laugh",
    u"\(\^J\^\)":"Normal Laugh",
    u"\(\*\^\.\^\*\)":"Normal Laugh",
    u"\(\^—\^\）":"Normal Laugh",
    u"\(#\^\.\^#\)":"Normal Laugh",
    u"\（\^—\^\）":"Waving",
    u"\(;_;\)/~~~":"Waving",
    u"\(\^\.\^\)/~~~":"Waving",
    u"\(-_-\)/~~~ \($\·\·\)/~~~":"Waving",
    u"\(T_T\)/~~~":"Waving",
    u"\(ToT\)/~~~":"Waving",
    u"\(\*\^0\^\*\)":"Excited",
    u"\(\*_\*\)":"Amazed",
    u"\(\*_\*;":"Amazed",
    u"\(\+_\+\) \(@_@\)":"Amazed",
    u"\(\*\^\^\)v":"Cheerful",
    u"\(\^_\^\)v":"Cheerful",
    u"\(\(d[-_-]b\)\)":"Listening to music",
    u'\(-"-\)':"Worried",
    u"\(ーー;\)":"Worried",
    u"\(\^0_0\^\)":"Eyeglasses",
    u"\(\＾ｖ\＾\)":"Happy",
    u"\(\＾ｕ\＾\)":"Happy",
    u"\(\^\)o\(\^\)":"Happy",
    u"\(\^O\^\)":"Happy",
    u"\(\^o\^\)":"Happy",
    u"\)\^o\^\(":"Happy",
    u":O o_O":"Surprised",
    u"o_0":"Surprised",
    u"o\.O":"Surpised",
    u"\(o\.o\)":"Surprised",
    u"oO":"Surprised",
    u"\(\*￣m￣\)":"Dissatisfied",
    u"\(‘A`\)":"Snubbed",
    u"<3":"Heart"
}

three_or_more_combos = [(r"%s{3,}" % (letter)) for letter in "abcdefghijklmnopqrstuvwxyz"]
three_or_more_re = r"(" + r"|".join(three_or_more_combos) + r")" 
    
one_or_more_combos = [(r"%s+" % letter) for letter in "abcdefghijklmnopqrstuvwxyz"]
one_or_more_re = r"(" + r"|".join(one_or_more_combos) + r")"

"""**replace characters in a particular column**"""

def char_replace(input, char_to_replace, char_to_replace_with, is_regex):
  return input.replace(to_replace=char_to_replace, value=char_to_replace_with, regex=is_regex)

"""**remove punctuation**"""

def remove_punctuation(comment):
  cleaned_comment = []
  for token in comment:
    if token not in string.punctuation:
      cleaned_comment.append(token)

  if(len(cleaned_comment) == 0):
      return comment
  
  return cleaned_comment

"""**remove stopwords**"""

def _remove_stopwords(comment, use_all):
  corrected_comment = []
  stopwords = ALL_STOPWORDS if use_all else SUB_STOPWORDS
  for token in comment:
    if token not in stopwords:
      corrected_comment.append(token)

  if(len(corrected_comment) == 0):
      return comment
  
  return corrected_comment

def remove_stopwords(df, col, use_all):
  df['stopwords_removed'] = df[col].apply(lambda x:_remove_stopwords(x, use_all))
  return df

"""**correct spellings in comment**"""

def correct_spellings(checker, comment):  
  corrected_comment = []
  all_emoticons = EMOTICONS.keys()
  emot_pattern = '(?:% s)' % '|'.join(all_emoticons)
  for token in comment:
    if token in ENG_EMOJI:  
      corrected_comment.append(token)    
      continue  
        
    if re.match(emot_pattern, token, re.UNICODE):        
        corrected_comment.append(token)
        continue

    misspelled_word = checker.unknown([token])
    if(len(misspelled_word) == 0):
      corrected_comment.append(token)
    else:
      corrected_comment.append(checker.correction(token))     

  return corrected_comment

"""**check if comment is English**"""

"""
def detect_comment_language_spacy(spacy_nlp, comment):
  corrected_comment = []
  for token in comment:
      if ( (token in ENG_EMOJI) or (re.match(r'@[\w]+', token)) or (re.match(r'#[\w]+', token)) ):
          pass
      else:
          corrected_comment.append(token)
  
  if len(corrected_comment) > 0:      
      comment_str =  ' '.join(token for token in corrected_comment)
      doc = spacy_nlp(comment_str) 
      detected_language = doc._.language 
      if len(detected_language) > 0:
          return detected_language['language']
  
  return "UNKNOWN"
"""

"""
def detect_comment_language_cld2(comment):
  corrected_comment = []
  for token in comment:
      if ( (token in ENG_EMOJI) or (re.match(r'@[\w]+', token)) or (re.match(r'#[\w]+', token)) ):
          pass
      else:
          corrected_comment.append(token)
  
  if len(corrected_comment) > 0:      
      comment_str =  ' '.join(token for token in corrected_comment)
      _, _, _, detected_language = cld2.detect(comment_str,  returnVectors=True)
      if len(detected_language) > 0:
          return detected_language[0][2]
  
  return "Unknown"
"""

"""**fix abbreviations and slangs**"""

def fix_abbr_slangs(comment):  
  corrected_comment = []
  for token in comment:   
    
    #if emoji, no correction needed
    if token in UNICODE_EMOJI:      
      corrected_comment.append(token)
      continue  
  
    replaced_token = [token]
    
    #check and fix urban slangs if present
    for patt, repl in urban_slangs_patt.items():
      is_match = re.match(patt, replaced_token[0])
      if is_match:            
        replaced_token = [repl]        
        break 
    
    if replaced_token[0] in abbreviations:
      replaced_token = abbreviations[replaced_token[0]]    
    
    corrected_comment.extend(replaced_token)

  return corrected_comment

"""**remove numbers**"""

def remove_numbers(comment):
  cleaned_comment = []
  for token in comment:
    if re.search(r'[0-9]+', token):
        pass
    else:
      cleaned_comment.append(token)

  return cleaned_comment

"""**correct spellings**"""
def spelling_correction(comment):
    corrected_comment = []
    for token in comment:
        corrected = Word(token).correct()
        corrected_comment.append(''.join(corrected))   
    return corrected_comment

"""**fix contractions**"""
def fix_contractions(comment):
    return contractions.fix(comment)    

"""**POS tag as needed by WordNetLemmatizer**"""
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)
  
"""**convert words in a sentence to their base form**"""
def lemmatize_comment(lemmatizer, comment):
    lemmatized_comment = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in  comment]
    return lemmatized_comment

def lemmatization(df, col):
    lemmatizer = WordNetLemmatizer()
    df['lemma_comment'] = df[col].apply(lambda x:lemmatize_comment(lemmatizer, x))
    return df  

"""**convert emoji's to their representation**"""
def _convert_emoji(comment, pattern_to_remove):
    converted_comment = []
    for token in comment:
        if token in ENG_EMOJI:
            emoji_rep = ENG_EMOJI[token]
            emoji_rep = re.sub(pattern_to_remove, "", emoji_rep)
            emoji_rep = emoji_rep.strip(":_")            
            converted_comment.append(emoji_rep)
        else:
            converted_comment.append(token)
    return converted_comment
    
def convert_emoji(df, col):
    pattern_to_remove = "dark_skin_tone|light_skin_tone|medium-dark_skin_tone|medium-light_skin_tone|medium_skin_tone"
    df['emoji_converted'] = df[col].apply(lambda x : _convert_emoji(x, pattern_to_remove))
    return df
    
"""**convert emoticons to their representation**"""
def _convert_emoticon(comment, emot_pattern):
    converted_comment = []
    for token in comment:
        if re.match(emot_pattern, token, re.UNICODE):
            for emot in EMOTICONS:
                token = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), token)
       
        converted_comment.append(token)
        
    return converted_comment  

def _convert_emoticon_trimmed(comment):
    converted_comment = []    
    for token in comment:
        rep_tokens = [token]
        for emot in EMOTICONS:
            if re.match(u'('+emot+')', token, re.UNICODE):
                rep_tokens = TRIMMED_EMOTICONS[emot].split()
                break            
        
       
        converted_comment.extend(rep_tokens)
        
    return converted_comment    

def convert_emoticon(df, col, full_rep):
    all_emoticons = EMOTICONS.keys()
    emot_pattern = '(?:% s)' % '|'.join(all_emoticons)
    if full_rep:
        df['emoticon_converted'] = df[col].apply(lambda x:_convert_emoticon(x, emot_pattern))   
    else:
        df['emoticon_converted'] = df[col].apply(lambda x:_convert_emoticon_trimmed(x))   
    return df

"""**fix words like faaakeee to fake**"""
def fix_repetition(comment):
    converted_comment = []
    for token in comment:
        if(re.findall(three_or_more_re, token)):
            letter_list = [letters[0] for letters in re.findall(three_or_more_re, token)]
            converted_comment.append("".join(letter_list))
        else:
            converted_comment.append(token)
    
    return converted_comment
    
def preprocess_data(df, col):  
  df[col] = df[col].replace(to_replace='’', value='\'', regex=True)

  #this is some character present in many comments and is not a whitespace
  df[col] = df[col].replace(to_replace='️', value='', regex=True)
  df[col] = df[col].replace(to_replace='\.{2,}', value='', regex=True)
  df[col] = df[col].replace(to_replace='…{1,}', value='', regex=True)
  df[col] = df[col].str.lower()
  df[col] = df[col].replace(to_replace='#\\w+', value='HASHTAG', regex=True)
  df[col] = df[col].replace(to_replace='@\\w+', value='USER', regex=True)
  df[col] = df[col].replace(to_replace='user\.\\w+', value='user', regex=True)
  df[col] = df[col].replace(to_replace='w\s+o\s+w', value='wow', regex=True)

  df['fixed_contractions'] = df[col].apply(lambda x:fix_contractions(x))
  tokenizer = TweetTokenizer()
  df['tokenized'] = df['fixed_contractions'].apply(lambda x:tokenizer.tokenize(x))
  df['punct_removed'] = df['tokenized'].apply(lambda x:remove_punctuation(x))
  
  #remove tokens that are numbers
  df['number_removed'] = df['punct_removed'].apply(lambda x:remove_numbers(x))  
  
  #df['language_cld2'] = df['number_removed'].apply(lambda x:detect_comment_language_cld2(x))  
  #filtered_df = df[df['language_cld2'].isin(['ENGLISH', 'Unknown'])] 
    
  df['fixed_abbr'] = df['number_removed'].apply(lambda x:fix_abbr_slangs(x))
  #filtered_df['fixed_rep'] = filtered_df['fixed_abbr'].apply(lambda x:fix_repetition(x))
  
  spell_checker = SpellChecker()
  df['spelling_corrected'] = df['fixed_abbr'].apply(lambda x:correct_spellings(spell_checker, x))
    
  return df



